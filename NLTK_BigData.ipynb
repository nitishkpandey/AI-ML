{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlcz1Twswwlt6691NUoL3z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitishkpandey/AI-ML/blob/main/NLTK_BigData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63e6a863",
        "outputId": "7d0e841b-fe43-47da-b16b-b4c969c1bb75"
      },
      "source": [
        "def tokenize_string_by_whitespace(text):\n",
        "  return text.split()\n",
        "\n",
        "first_string = \"I am a Boy who goes to the school daily.\"\n",
        "tokenized_words = tokenize_string_by_whitespace(first_string)\n",
        "print(tokenized_words)\n",
        "\n",
        "string_two = \" I am here to play football.\"\n",
        "tokenized_words_two = tokenize_string_by_whitespace(string_two)\n",
        "print(tokenized_words_two)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'a', 'Boy', 'who', 'goes', 'to', 'the', 'school', 'daily.']\n",
            "['I', 'am', 'here', 'to', 'play', 'football.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: Write a python program that uses the NLTK library to perform word level tokeization."
      ],
      "metadata": {
        "id": "h8GIX7R0sJ_z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69ef9f14",
        "outputId": "bbda3bb4-d0f6-46d5-ce76-e29c7dfb4dbb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a2c647e",
        "outputId": "9495af78-2629-4b49-c3a7-5e0b38f6d419"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"NLTK is a powerful library for natural language processing.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b740703a",
        "outputId": "69ae76cd-765e-4333-a413-540537546684"
      },
      "source": [
        "another_string = \"It's a beautiful day, isn't it?\"\n",
        "another_nltk_tokens = word_tokenize(another_string)\n",
        "print(another_nltk_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', \"'s\", 'a', 'beautiful', 'day', ',', 'is', \"n't\", 'it', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: Write a python function that performs character-level tokenization of a string."
      ],
      "metadata": {
        "id": "sV2DuYWIvSOk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88564970",
        "outputId": "16e6c306-a5fa-44a6-c5c1-14cce8c4078f"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "def character_tokenize_nltk(text):\n",
        "  return regexp_tokenize(text, r'.')\n",
        "\n",
        "text_nltk = \"Hello, This is Nitish!\"\n",
        "nltk_char_tokens = character_tokenize_nltk(text_nltk)\n",
        "print(f\"Original text: '{text_nltk}'\")\n",
        "print(f\"Character tokens: {nltk_char_tokens}\")\n",
        "\n",
        "another_text_nltk = \"I am studying Data Science\"\n",
        "another_nltk_char_tokens = character_tokenize_nltk(another_text_nltk)\n",
        "print(f\"Original text: '{another_text_nltk}'\")\n",
        "print(f\"Character tokens: {another_nltk_char_tokens}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: 'Hello, This is Nitish!'\n",
            "Character tokens: ['H', 'e', 'l', 'l', 'o', ',', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'N', 'i', 't', 'i', 's', 'h', '!']\n",
            "Original text: 'I am studying Data Science'\n",
            "Character tokens: ['I', ' ', 'a', 'm', ' ', 's', 't', 'u', 'd', 'y', 'i', 'n', 'g', ' ', 'D', 'a', 't', 'a', ' ', 'S', 'c', 'i', 'e', 'n', 'c', 'e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: Write a python program that builds a byte pair encoding tokenizer using the hugging face tokenizers library."
      ],
      "metadata": {
        "id": "vYcWl0785qAa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22435e19",
        "outputId": "e862322e-2f99-4b7a-e3d4-94228833dea6"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "text_data = [\n",
        "    \"Hello, world! This is an example sentence for BPE tokenization.\",\n",
        "    \"Byte Pair Encoding is a popular method for subword tokenization.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Hugging Face provides excellent libraries for NLP.\"\n",
        "]\n",
        "\n",
        "trainer = BpeTrainer(\n",
        "    vocab_size=100,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"] # Special tokens\n",
        ")\n",
        "\n",
        "tokenizer.train_from_iterator(text_data, trainer=trainer)\n",
        "\n",
        "tokenizer.save(\"bpe_tokenizer.json\")\n",
        "print(\"Tokenizer trained and saved as bpe_tokenizer.json\")\n",
        "\n",
        "loaded_tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "sentence_to_encode = \"Hugging Face tokenizers are very efficient.\"\n",
        "encoded_output = loaded_tokenizer.encode(sentence_to_encode)\n",
        "\n",
        "print(f\"\\nOriginal sentence: '{sentence_to_encode}'\")\n",
        "print(f\"Encoded tokens: {encoded_output.tokens}\")\n",
        "print(f\"Encoded IDs: {encoded_output.ids}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer trained and saved as bpe_tokenizer.json\n",
            "\n",
            "Original sentence: 'Hugging Face tokenizers are very efficient.'\n",
            "Encoded tokens: ['H', 'u', 'g', 'g', 'ing', 'F', 'a', 'ce', 'token', 'iz', 'e', 'r', 's', 'ar', 'e', 'v', 'e', 'r', 'y', 'e', 'f', 'f', 'i', 'c', 'i', 'ent', '.']\n",
            "Encoded IDs: [11, 36, 22, 22, 67, 10, 16, 44, 63, 55, 20, 33, 34, 48, 20, 37, 20, 33, 40, 20, 21, 21, 24, 18, 24, 65, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f38d4657"
      },
      "source": [
        "### Explanation:\n",
        "\n",
        "1.  **`pip install tokenizers`**: Installs the necessary Hugging Face `tokenizers` library.\n",
        "2.  **`Tokenizer(BPE())`**: Initializes a new tokenizer with the Byte Pair Encoding model.\n",
        "3.  **`tokenizer.pre_tokenizer = Whitespace()`**: Sets the pre-tokenization strategy, which initially splits the text into words based on whitespace.\n",
        "4.  **`text_data`**: A list of strings used to train the BPE model. In a real-world scenario, this would be a much larger corpus.\n",
        "5.  **`BpeTrainer(...)`**: Configures the BPE training process. Key parameters include:\n",
        "    *   `vocab_size`: The desired size of the final vocabulary.\n",
        "    *   `min_frequency`: The minimum frequency a pair of tokens must have to be merged.\n",
        "    *   `special_tokens`: A list of tokens that should not be merged or broken down.\n",
        "6.  **`tokenizer.train_from_iterator(text_data, trainer=trainer)`**: Trains the BPE model using the provided text data and trainer configuration.\n",
        "7.  **`tokenizer.save(\"bpe_tokenizer.json\")`**: Saves the trained tokenizer to a JSON file, which can be easily reloaded later.\n",
        "8.  **`Tokenizer.from_file(\"bpe_tokenizer.json\")`**: Loads a tokenizer from a saved file.\n",
        "9.  **`loaded_tokenizer.encode(sentence_to_encode)`**: Encodes a given sentence using the trained tokenizer, returning an `Encoding` object that contains the tokens, their IDs, offsets, and more.\n",
        "\n",
        "This process creates a BPE tokenizer that learns common subword units from the training data, allowing it to handle out-of-vocabulary words gracefully by breaking them into known subwords."
      ]
    }
  ]
}