{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSulK1E4oZ0o08BeH14ELn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitishkpandey/AI-ML/blob/main/Text_Correction_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Calculate the edit distance between \"ABCD\" and \"AECDB\""
      ],
      "metadata": {
        "id": "uR-Gaiv2wf9X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51818793",
        "outputId": "5ffc34c7-21d6-4f84-95e6-9fd705c6c98e"
      },
      "source": [
        "def levenshtein_distance(s1, s2):\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein_distance(s2, s1)\n",
        "\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "\n",
        "    previous_row = range(len(s2) + 1)\n",
        "\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "\n",
        "    return previous_row[-1]\n",
        "\n",
        "string1 = \"ABCD\"\n",
        "string2 = \"AECDB\"\n",
        "distance = levenshtein_distance(string1, string2)\n",
        "print(f\"The edit distance between '{string1}' and '{string2}' is: {distance}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The edit distance between 'ABCD' and 'AECDB' is: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Using the TextBlob library in python, write a program that performs spelling correction on a sentence."
      ],
      "metadata": {
        "id": "IBP7DLaDyyQ1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bc40434",
        "outputId": "813c2f34-87a4-43de-947d-ad663d661810"
      },
      "source": [
        "!pip install -U textblob"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.12/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0baf4e9e",
        "outputId": "70863184-9832-40e4-b8f2-fea2f11b4a04"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "sentence = input(\" Enter an incorrect sentence \")\n",
        "print(f\"Original: {sentence}\")\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "\n",
        "corrected_sentence = blob.correct()\n",
        "\n",
        "print(f\"Corrected sentence: {corrected_sentence}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Enter an incorrect sentences I am a boi\n",
            "Original: I am a boi\n",
            "Corrected sentence: I am a boy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Write a program that demonstrate the difference between stemming and lemmatization using NLTK library."
      ],
      "metadata": {
        "id": "pASNVpq__aaV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd12ac26",
        "outputId": "70a5cf99-13ac-4053-d669-4cadc058f505"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16a73718",
        "outputId": "fd98a44a-c31a-46eb-bfd2-2e2600bdd079"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"running runs runner ran quickly better cats mice\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "\n",
        "print(f\"Original words: {words}\\n\")\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(f\"Stemmed words (PorterStemmer): {stemmed_words}\")\n",
        "print(\"Stemming reduces words to their root form, which may not be a dictionary word. For example, 'running', 'runs', 'runner', 'ran' all become 'run'.\\n\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(f\"Lemmatized words (WordNetLemmatizer): {lemmatized_words}\")\n",
        "print(\"Lemmatization reduces words to their base or dictionary form (lemma). For example, 'running' becomes 'running', but 'better' becomes 'good' and 'mice' becomes 'mouse', 'running' might not be lemmatized to 'run' as it defaults to noun.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'runs', 'runner', 'ran', 'quickly', 'better', 'cats', 'mice']\n",
            "\n",
            "Stemmed words (PorterStemmer): ['run', 'run', 'runner', 'ran', 'quickli', 'better', 'cat', 'mice']\n",
            "Stemming reduces words to their root form, which may not be a dictionary word. For example, 'running', 'runs', 'runner', 'ran' all become 'run'.\n",
            "\n",
            "Lemmatized words (WordNetLemmatizer): ['running', 'run', 'runner', 'ran', 'quickly', 'better', 'cat', 'mouse']\n",
            "Lemmatization reduces words to their base or dictionary form (lemma). For example, 'running' becomes 'running', but 'better' becomes 'good' and 'mice' becomes 'mouse', 'running' might not be lemmatized to 'run' as it defaults to noun.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f34f4bf",
        "outputId": "d5193a67-d7c9-489f-c356-6add0efc840d"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text = \"running runs runner ran quickly better cats mice\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "\n",
        "print(f\"Original words: {words}\\n\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words_pos = []\n",
        "for word, tag in pos_tag(words):\n",
        "    wntag = tag[0].lower()\n",
        "    wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
        "    if not wntag:\n",
        "        lemma = lemmatizer.lemmatize(word)\n",
        "    else:\n",
        "        lemma = lemmatizer.lemmatize(word, wntag)\n",
        "    lemmatized_words_pos.append(lemma)\n",
        "\n",
        "print(f\"Lemmatized words: {lemmatized_words_pos}\")\n",
        "print(\"Lemmatization with POS tagging provides more accurate results by considering the word's grammatical role. For instance, 'running' (as a verb) is correctly lemmatized to 'run', and 'better' (as an adjective) is lemmatized to 'good'.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'runs', 'runner', 'ran', 'quickly', 'better', 'cats', 'mice']\n",
            "\n",
            "Lemmatized words: ['run', 'run', 'runner', 'run', 'quickly', 'better', 'cat', 'mice']\n",
            "Lemmatization with POS tagging provides more accurate results by considering the word's grammatical role. For instance, 'running' (as a verb) is correctly lemmatized to 'run', and 'better' (as an adjective) is lemmatized to 'good'.\n"
          ]
        }
      ]
    }
  ]
}