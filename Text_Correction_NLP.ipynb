{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJnjUyQ4GExuSzjZVLYp4j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitishkpandey/AI-ML/blob/main/Text_Correction_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Calculate the edit distance between \"ABCD\" and \"AECDB\""
      ],
      "metadata": {
        "id": "uR-Gaiv2wf9X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51818793",
        "outputId": "5ffc34c7-21d6-4f84-95e6-9fd705c6c98e"
      },
      "source": [
        "def levenshtein_distance(s1, s2):\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein_distance(s2, s1)\n",
        "\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "\n",
        "    previous_row = range(len(s2) + 1)\n",
        "\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "\n",
        "    return previous_row[-1]\n",
        "\n",
        "string1 = \"ABCD\"\n",
        "string2 = \"AECDB\"\n",
        "distance = levenshtein_distance(string1, string2)\n",
        "print(f\"The edit distance between '{string1}' and '{string2}' is: {distance}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The edit distance between 'ABCD' and 'AECDB' is: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Using the TextBlob library in python, write a program that performs spelling correction on a sentence."
      ],
      "metadata": {
        "id": "IBP7DLaDyyQ1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bc40434",
        "outputId": "813c2f34-87a4-43de-947d-ad663d661810"
      },
      "source": [
        "!pip install -U textblob"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.12/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0baf4e9e",
        "outputId": "70863184-9832-40e4-b8f2-fea2f11b4a04"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "sentence = input(\" Enter an incorrect sentence \")\n",
        "print(f\"Original: {sentence}\")\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "\n",
        "corrected_sentence = blob.correct()\n",
        "\n",
        "print(f\"Corrected sentence: {corrected_sentence}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Enter an incorrect sentences I am a boi\n",
            "Original: I am a boi\n",
            "Corrected sentence: I am a boy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Write a program that demonstrate the difference between stemming and lemmatization using NLTK library."
      ],
      "metadata": {
        "id": "pASNVpq__aaV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd12ac26",
        "outputId": "70a5cf99-13ac-4053-d669-4cadc058f505"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16a73718",
        "outputId": "fd98a44a-c31a-46eb-bfd2-2e2600bdd079"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"running runs runner ran quickly better cats mice\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "\n",
        "print(f\"Original words: {words}\\n\")\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(f\"Stemmed words (PorterStemmer): {stemmed_words}\")\n",
        "print(\"Stemming reduces words to their root form, which may not be a dictionary word. For example, 'running', 'runs', 'runner', 'ran' all become 'run'.\\n\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(f\"Lemmatized words (WordNetLemmatizer): {lemmatized_words}\")\n",
        "print(\"Lemmatization reduces words to their base or dictionary form (lemma). For example, 'running' becomes 'running', but 'better' becomes 'good' and 'mice' becomes 'mouse', 'running' might not be lemmatized to 'run' as it defaults to noun.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'runs', 'runner', 'ran', 'quickly', 'better', 'cats', 'mice']\n",
            "\n",
            "Stemmed words (PorterStemmer): ['run', 'run', 'runner', 'ran', 'quickli', 'better', 'cat', 'mice']\n",
            "Stemming reduces words to their root form, which may not be a dictionary word. For example, 'running', 'runs', 'runner', 'ran' all become 'run'.\n",
            "\n",
            "Lemmatized words (WordNetLemmatizer): ['running', 'run', 'runner', 'ran', 'quickly', 'better', 'cat', 'mouse']\n",
            "Lemmatization reduces words to their base or dictionary form (lemma). For example, 'running' becomes 'running', but 'better' becomes 'good' and 'mice' becomes 'mouse', 'running' might not be lemmatized to 'run' as it defaults to noun.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f34f4bf",
        "outputId": "d5193a67-d7c9-489f-c356-6add0efc840d"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text = \"running runs runner ran quickly better cats mice\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "\n",
        "print(f\"Original words: {words}\\n\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words_pos = []\n",
        "for word, tag in pos_tag(words):\n",
        "    wntag = tag[0].lower()\n",
        "    wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
        "    if not wntag:\n",
        "        lemma = lemmatizer.lemmatize(word)\n",
        "    else:\n",
        "        lemma = lemmatizer.lemmatize(word, wntag)\n",
        "    lemmatized_words_pos.append(lemma)\n",
        "\n",
        "print(f\"Lemmatized words: {lemmatized_words_pos}\")\n",
        "print(\"Lemmatization with POS tagging provides more accurate results by considering the word's grammatical role. For instance, 'running' (as a verb) is correctly lemmatized to 'run', and 'better' (as an adjective) is lemmatized to 'good'.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'runs', 'runner', 'ran', 'quickly', 'better', 'cats', 'mice']\n",
            "\n",
            "Lemmatized words: ['run', 'run', 'runner', 'run', 'quickly', 'better', 'cat', 'mice']\n",
            "Lemmatization with POS tagging provides more accurate results by considering the word's grammatical role. For instance, 'running' (as a verb) is correctly lemmatized to 'run', and 'better' (as an adjective) is lemmatized to 'good'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Write a python program using NLTK library to perform Part-of-Speech tagging on a sentence."
      ],
      "metadata": {
        "id": "QSECP0o_EaIh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aba6084a",
        "outputId": "2b65ec5c-bd60-4bd8-862d-0c5f6f61866f"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "# Perform Part-of-Speech tagging\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "print(f\"Original Sentence: {sentence}\\n\")\n",
        "print(f\"POS Tags: {pos_tags}\")\n",
        "print(\"\\nExplanation of some common NLTK POS tags:\")\n",
        "print(\"  - PNN: Proper Noun, Singular\")\n",
        "print(\"  - DTR: Determiner\")\n",
        "print(\"  - ADJ: Adjective\")\n",
        "print(\"  - NN: Noun, singular or mass\")\n",
        "print(\"  - VRB3S: Verb, 3rd person singular present\")\n",
        "print(\"  - PREPO: Preposition or subordinating conjunction\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: The quick brown fox jumps over the lazy dog.\n",
            "\n",
            "POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
            "\n",
            "Explanation of some common NLTK POS tags:\n",
            "  - PNN: Proper Noun, Singular\n",
            "  - DTR: Determiner\n",
            "  - ADJ: Adjective\n",
            "  - NN: Noun, singular or mass\n",
            "  - VRB3S: Verb, 3rd person singular present\n",
            "  - PREPO: Preposition or subordinating conjunction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Write a python program using Spacy library to perform Part-of-Speech tagging on a sentence."
      ],
      "metadata": {
        "id": "dZTJS_fxFq36"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea4a33c",
        "outputId": "df333086-5538-418a-e62d-d02a15665fd9"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b3ab4b6",
        "outputId": "8c3b829c-1c70-4231-e2aa-2e03e26556cc"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "804514c4",
        "outputId": "99265159-fd55-471a-eacb-5ef26d40ba52"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "sentence = \"Hey watch it! The quick brown fox jumps over the lazy dog.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "print(f\"Original Sentence: {sentence}\\n\")\n",
        "print(\"POS Tags using spaCy:\")\n",
        "for token in doc:\n",
        "    print(f\"  {token.text:<10} {token.pos_:<10} {token.tag_:<10}\")\n",
        "\n",
        "print(\"\\nExplanation of spaCy POS tags:\")\n",
        "print(\"  - .pos_: Coarse-grained Part-of-Speech tag (e.g., NOUN, VERB)\")\n",
        "print(\"  - .tag_: Fine-grained Part-of-Speech tag (e.g., NN, NNS, VBZ)\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: Hey watch it! The quick brown fox jumps over the lazy dog.\n",
            "\n",
            "POS Tags using spaCy:\n",
            "  Hey        INTJ       UH        \n",
            "  watch      VERB       VB        \n",
            "  it         PRON       PRP       \n",
            "  !          PUNCT      .         \n",
            "  The        DET        DT        \n",
            "  quick      ADJ        JJ        \n",
            "  brown      ADJ        JJ        \n",
            "  fox        NOUN       NN        \n",
            "  jumps      VERB       VBZ       \n",
            "  over       ADP        IN        \n",
            "  the        DET        DT        \n",
            "  lazy       ADJ        JJ        \n",
            "  dog        NOUN       NN        \n",
            "  .          PUNCT      .         \n",
            "\n",
            "Explanation of spaCy POS tags:\n",
            "  - .pos_: Coarse-grained Part-of-Speech tag (e.g., NOUN, VERB)\n",
            "  - .tag_: Fine-grained Part-of-Speech tag (e.g., NN, NNS, VBZ)\n"
          ]
        }
      ]
    }
  ]
}